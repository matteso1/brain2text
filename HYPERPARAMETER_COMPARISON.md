# Hyperparameter Comparison: Your Config vs Official

## Critical Differences (Explain Instability)

| Parameter | Your Value | Official Value | Impact |
|-----------|------------|----------------|--------|
| **white_noise_std** | 0.25 | **1.0** | ðŸ”´ 4x weaker augmentation â†’ blank collapse |
| **constant_offset_std** | 0.05 | **0.2** | ðŸ”´ 4x weaker augmentation â†’ blank collapse |
| **lr_max** | 0.002 | **0.005** | ðŸ”´ 2.5x lower LR â†’ too slow with epsilon=0.1 |
| **lr_warmup_steps** | 2000 | **1000** | ðŸŸ¡ 2x longer warmup â†’ slower start |
| **batch_size** | 48 | **64** | ðŸŸ¡ 25% smaller batches â†’ slightly different gradients |
| **epsilon** | 0.1 âœ… | 0.1 | âœ… Correct (after fixing) |

## Other Settings (All Correct)

| Parameter | Your Value | Official Value | Status |
|-----------|------------|----------------|--------|
| Model architecture | 5-layer GRU, 768 units | Same | âœ… |
| Model params | 44.32M | Same | âœ… |
| Dropout | 0.4 (GRU), 0.2 (input) | Same | âœ… |
| Patch size/stride | 14/4 | Same | âœ… |
| Weight decay | 0.001 (main), 0.0 (day) | Same | âœ… |
| Grad clip | 10.0 | Same | âœ… |
| Beta0/Beta1 | 0.9/0.999 | Same | âœ… |
| CTC blank | 0 | Same | âœ… |
| Smoothing kernel | 100 size, 2 std | Same | âœ… |
| Random cut | 3 | Same | âœ… |
| Seed | 10 | Same | âœ… |

## Impact Analysis

### Why Blank Collapse Happened

With your settings (epsilon=0.1, weak augmentation, low LR):

```
Step 1: Model sees relatively clean data (weak noise)
Step 2: CTC loss is minimized by predicting blank (always safe)
Step 3: Low LR (0.002) + epsilon=0.1 â†’ very small effective steps
Step 4: Model gets stuck in blank prediction mode
Step 5: 100% blank predictions, loss diverges
```

### Why Official Settings Work

With official settings (epsilon=0.1, strong augmentation, high LR):

```
Step 1: Model sees very noisy data (4x stronger augmentation)
Step 2: Must learn robust phoneme features (blank isn't always best)
Step 3: High LR (0.005) + epsilon=0.1 â†’ reasonable effective steps
Step 4: Strong regularization prevents overfitting/mode collapse
Step 5: Stable training to ~10% PER
```

## Recommended Action

**Use the official exact config:**
```bash
python src/train_baseline.py --config configs/rnn_official_exact.yaml --num-batches 10000
```

This should give you:
- âœ… Stable training (no gradient explosions)
- âœ… No blank collapse (strong augmentation prevents it)
- âœ… ~10-12% PER after 10k batches
- âœ… ~8-10% PER after full 120k training

## Optional: Verify Hypothesis

After confirming official settings work, you can verify the augmentation hypothesis:

1. **Keep official config, but reduce augmentation to your original values**
   - Expected: Blank collapse returns
   - Proves: Strong augmentation is critical with epsilon=0.1

2. **Keep official config, but reduce LR to 0.002**
   - Expected: Much slower convergence or blank collapse
   - Proves: Higher LR needed with epsilon=0.1

## Summary

You did **excellent debugging** and fixed all the code bugs:
- âœ… Bias weight decay
- âœ… Data augmentation order
- âœ… Validation smoothing
- âœ… Model architecture

The remaining issue was **hyperparameter mismatch**, specifically:
- 4x weaker data augmentation
- 2.5x lower learning rate

These interact critically with epsilon=0.1 to cause blank collapse.

